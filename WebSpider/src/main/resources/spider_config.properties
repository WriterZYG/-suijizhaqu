#global config
node_is_master=false
#node_is_master=false
#最大的种子的url的个数
node_seeds_max_size=100000000
#一次性注入到redis的url任务的数量，即批量添加的条数
node_seeds_inject_batch_size=1000
#低于这个值就会自动注入新的集合到redis队列中
node_redis_size_threshold=1000
#注入一次后sleep多长时间
node_inject_urls_sleep_time=5000
#client成功抓取一个URL的休息时间
node_client_grab_success_sleep_interval_time=500
#请求失败后，可以重复请求的次数
node_req_fail_max_count=2

#httpclient  parameters
HttpGet.http.socket.timeout=30000
#最大链接数
max_connections=80
#获取链接的最大等待时间
wait_connection_timeout=15000
#链接超时时间
connection_timeout=100000
#读数据超时时间
read_timeout=30000
#遇到错误时候的最大重复请求次数
http_req_error_repeat_number=3
http_req_once_wait_time=300

#爬虫的参数设置
#爬虫种子目录，暂以文件形式来管理种子任务
spider_seeds_root_path=seeds
#为快捷简单,暂不启用该参数
threads=2
#一些数据暂存到本地文件系统中
data_dir=data
depth=4
#depth=0
max_depth=6
#产生二级任务量
topN=400
max_topN=600

proxy_open=false
#proxy_open=true
#代表是否启用自身IP去请求下载网页
proxy_self=true
#proxy_self=false

ip_proxy_file_path=proxy
proxy_fail_max_count=1

#crawl repeat config
crawl_page_repeat_number=1

#log config
#log_output_file_enable=true
log_output_file_enable=false

#web service binding defination
ws_server_ip=127.0.0.1
ws_server_port=9999

#关于redis的server host ip 和 port的配置
#redis_host=127.0.0.1
redis_host=127.0.0.1
redis_port=6379
redis_password=root

#为simple crawler添加的新配置
#表明规则文件所在的位置，是在jar包，还是在jar之外的文件系统中.in为在jar包中，out则是在文件文件系统中
#ext_content_rule_config_fs=in
ext_content_rule_config_fs=out
#此处可以写文件，也可以选择是文件夹。如果上边是in,则此处只支持文件，外部文件系统的话就无所谓了
#规则模板所存放的总目录，里边可以随意存文件或文件夹，只要符合规则即可
ext_content_rule_config_root_dir=rules

ext_content_save_threads_numbers=1
ext_content_save_thread_sleep_time=10000
#0代表没有更新，1代表有更新
ext_content_rule_key_is_changed=1
#时间为s
ext_content_rule_key_syn_circle=10
#是否开启循环加载种子文件
ext_content_load_seeds_is_circle=false
#循环加载种子目录时，多长时间加载一次!
ext_content_load_seeds_sleep=10

#cache data config
#unit is second
cache_data_circle_save_interval=60

#task parameters
task_circle_enable=true

#####phnatomjs start##################################
phantomjs_path=phantomjs/
phantomjs_exe_name=phantomjs
phantomjs_crawl_config_jquery_path=phantomjs/jquery-2.1.0.min.js
phantomjs_crawl_config_json_root_path=phantomjs/config_phantomjs/
phantomjs_crawl_config_js_root_path=phantomjs/js/
phantomjs_crawl_config_js_para_root_path=phantomjs/para/
#crawl param config
phantomjs_crawl_config_capture_pic_root_path_seg=phantomjs/capture/
phantomjs_crawl_config_capture_pic_root_path_body=phantomjs/capture_body/
phantomjs_crawl_config_txt_root_path_seg=phantomjs/crawlData/
phantomjs_crawl_config_txt_root_path_body=phantomjs/crawlData_body/

phantomjs_crawl_config_all_data_root_path_random_url=phantomjs/random_url/

phantomjs_crawl_config_no_response_waitting_time_max=80000
phantomjs_crawl_config_no_response_waitting_fail_time_max=2

#static value is to here
is_inject_jquery_default=true
#is_inject_jquery_default=false
is_capture_pic_default_seg=true
#is_capture_pic_default_seg=false
#is_capture_pic_default_body=true
is_capture_pic_default_body=false
is_capture_pic_default_random_page=true
#is_capture_pic_default_random_page=false
is_data_write_to_file_default=true
#元搜索抓取的页数
crawl_max_page_number=3

#抓取输出定义
phantojs_seg_output_root_path=crawl_output/
#####phnatomjs end##################################

#######Test########
test=成功
